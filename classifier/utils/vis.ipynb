{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 用于可视化该数据集，理解标注文件的含义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_path = Path(\"Coffee_room_01/Annotation_files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int(x, y):\n",
    "    return int(x), int(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_videos(ann_path):\n",
    "    try:\n",
    "        video_path = ann_path.parents[0].joinpath(\"Videos\")\n",
    "        cv2.namedWindow(\"videos\", cv2.WINDOW_KEEPRATIO)\n",
    "        print(\"Press 'q' to exit this function.\")\n",
    "        for ann in ann_path.glob(\"*.txt\"):\n",
    "            # print(f\"{str(ann)}\")\n",
    "            with ann.open('r') as fd:\n",
    "                lines = fd.readlines()\n",
    "                lines = [l.strip('\\n') for l in lines]\n",
    "            fall_frame_start = int(lines[0])  # 跌倒动作起始帧\n",
    "            fall_frame_end = int(lines[1])    # 跌倒动作结束帧， 跌倒动作后躺地上的帧数\n",
    "            frame_info = [l.replace(',', ' ') for l in lines[2:]]\n",
    "            frame_info = np.loadtxt(frame_info)   \n",
    "\n",
    "            video_file = video_path.joinpath(ann.stem + \".avi\")\n",
    "            cap = cv2.VideoCapture(str(video_file))\n",
    "            frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "            # fall_direction 是跌倒方向，顺时针从1-8, 1是正北(默认值), 2是东北, 3是正东, ....\n",
    "            for frame_id, fall_direction, x1, y1, x2, y2 in frame_info:\n",
    "                if cap.isOpened() and frame_id < frame_count:\n",
    "                    _, img = cap.read()\n",
    "\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    img = cv2.rectangle(img, _int(x1, y1), _int(x2, y2), color=(0, 255, 0), thickness=1)\n",
    "                    text = f\"{frame_id:2.0f}, {fall_direction:2.0f}\"\n",
    "                    img = cv2.putText(img, text, (14, 17), cv2.FONT_HERSHEY_SIMPLEX, 0.5,  color=(0, 255, 0), thickness=1)\n",
    "                    \n",
    "                    if fall_frame_start <= frame_id <= fall_frame_end:\n",
    "                        img = cv2.putText(img, \"falling\", (14, 34), cv2.FONT_HERSHEY_SIMPLEX, 0.5,  color=(0, 0, 255), thickness=1)\n",
    "                    elif fall_direction == 1:\n",
    "                        img = cv2.putText(img, \"normal\", (14, 34), cv2.FONT_HERSHEY_SIMPLEX, 0.5,  color=(0, 255, 0), thickness=1)\n",
    "                    elif fall_direction != 1 or frame_id > fall_frame_end:\n",
    "                        img = cv2.putText(img, \"faint\", (14, 34), cv2.FONT_HERSHEY_SIMPLEX, 0.5,  color=(255, 255, 0), thickness=1)\n",
    "                        \n",
    "                    cv2.imshow(\"videos\", img)\n",
    "                    key = cv2.waitKey(1)\n",
    "                    if key & 0XFFFF == ord('q'):\n",
    "                        return\n",
    "            cap.release()\n",
    "    except ValueError: \n",
    "        print(f\"value error: ann_file => {ann}\")\n",
    "    finally:\n",
    "        cv2.destroyAllWindows()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to exit this function.\n"
     ]
    }
   ],
   "source": [
    "show_videos(ann_path=ann_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试使用sklearn进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        json_dict = json.load(f)\n",
    "    return json_dict\n",
    "\n",
    "def save_json(json_obj, json_file):\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(json_obj, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'normal 0 | falling 1 | faint 2', 'fall_direction': 'int 1~8', 'bbox': 'x1, y1, w, h', 'keypoints': 'x1, y1, v1, ...., x12, y12, v12', 'keypoints_order': ['left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'right_hip', 'left_knee', 'right_knee', 'left_ankle', 'right_ankle'], 'skeleton': [[0, 1], [6, 7], [0, 2], [1, 3], [2, 4], [3, 5], [0, 6], [1, 7], [6, 8], [7, 9], [8, 10], [9, 11]]}\n"
     ]
    }
   ],
   "source": [
    "ann_dict = load_json(\"FallDataset/Coffee_room_01/annotations/video (1).json\")\n",
    "print(f\"{ann_dict['info']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_from_json(json_file):\n",
    "    ann_dict = load_json(str(json_file))\n",
    "    kpts, bboxes, labels = [], [], []\n",
    "    for ann in ann_dict['annotations']:\n",
    "        kpt = ann['keypoints']\n",
    "        bbox = ann['bbox']\n",
    "        if kpt != [] and bbox != [] \\\n",
    "            and bbox[2] > 0 and bbox[3] > 0:\n",
    "            kpts.append(kpt)\n",
    "            bboxes.append(bbox)\n",
    "            labels.append(ann['label'])\n",
    "\n",
    "    num_frame = len(kpts)\n",
    "    bboxes = np.array(bboxes)\n",
    "    w, h = bboxes[:, 2], bboxes[:, 3]\n",
    "    ratio = w / h  # w/h\n",
    "\n",
    "    kpts = np.array(kpts).reshape((num_frame, -1, 3))\n",
    "    kpts[:, :, 0] /= w[:, None] \n",
    "    kpts[:, :, 1] /= h[:, None] \n",
    "\n",
    "    x = np.concatenate([kpts.reshape((num_frame, -1)), ratio[:, None]], axis=1)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载标注文件，得到输入X和输出Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(annotations_dirs):\n",
    "    if not isinstance(annotations_dirs, list):\n",
    "        annotations_dirs = [annotations_dirs]\n",
    "    X, Y = [], []\n",
    "    for ann_dir in tqdm(annotations_dirs):\n",
    "        ann_dir = Path(ann_dir)\n",
    "        x, y = [], []\n",
    "        for json_file in ann_dir.glob(\"*.json\"):\n",
    "            _x, _y = get_xy_from_json(json_file)\n",
    "            x.append(_x)\n",
    "            y.append(_y)\n",
    "        X.append(np.concatenate(x, axis=0))\n",
    "        Y.append(np.concatenate(y, axis=0))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_data(annotations_dirs=[\n",
    "    \"FallDataset/Coffee_room_01/annotations\",\n",
    "    \"FallDataset/Coffee_room_02/annotations\",\n",
    "    \"FallDataset/Home_01/annotations\",\n",
    "    \"FallDataset/Home_02/annotations\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析各类别样本数和分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8780,  1394,  4001],\n",
       "       [10362,   411,  1814],\n",
       "       [ 2542,   418,  3006],\n",
       "       [ 4828,    93,  1073]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_class_samples(Y, num_class=3):\n",
    "    num_all = []\n",
    "    if isinstance(Y, list):\n",
    "        for y in Y:\n",
    "            num = []\n",
    "            for i in range(num_class):\n",
    "                num.append(sum(y==i))\n",
    "            num_all.append(num)\n",
    "    else:\n",
    "        for i in range(num_class):\n",
    "            num_all.append(sum(Y==i))\n",
    "    return np.array(num_all)\n",
    "\n",
    "num_samples = count_class_samples(Y)\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61940035, 0.09834215, 0.2822575 ],\n",
       "       [0.82323032, 0.03265274, 0.14411695],\n",
       "       [0.42608113, 0.07006369, 0.50385518],\n",
       "       [0.80547214, 0.01551552, 0.17901235]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个视频目录的类别比例\n",
    "percentage_per_dir = num_samples / num_samples.sum(axis=1)[:, None]\n",
    "percentage_per_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([26512,  2316,  9894]), array([0.68467538, 0.05981096, 0.25551366]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 总的类别比例\n",
    "num_samples_all = num_samples.sum(axis=0)\n",
    "percentage_all = num_samples_all / num_samples_all.sum()\n",
    "num_samples_all, percentage_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到总的输入样本和输出样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = np.concatenate(X, axis=0)\n",
    "y_all = np.concatenate(Y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O、划分数据集（train/test）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、 简单随机抽样\n",
    "+ 在所有样本中按比例随机抽取，有小概率导致划分结果的类别分布变化较大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.3)\n",
    "[data.shape for data in [x_train, x_test, y_train, y_test]], pd.value_counts(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、分层抽样\n",
    "+ 分别在各类别中按比例划分，确保划分数据集前后，类别分布基本不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(27105, 37), (11617, 37), (27105,), (11617,)],\n",
       " 0    18558\n",
       " 2     6926\n",
       " 1     1621\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.3, stratify=y_all)\n",
    "[data.shape for data in [x_train, x_test, y_train, y_test]], pd.value_counts(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、过采样策略——数据不平衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over sampler: {0: 18558, 1: 1621, 2: 6926} => {0: 18558, 1: 14846, 2: 14846}\n"
     ]
    }
   ],
   "source": [
    "# 通过采样策略解决数据样本不均衡问题\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, ADASYN\n",
    "num_train = count_class_samples(y_train)   # 各类别的训练样本数\n",
    "num_original = {i:int(num_train[i]) for i in range(3)} # 总数据集各类别样本数\n",
    "sampling_strategy  = dict()\n",
    "max_samples = max(num_train)\n",
    "over_ratio = 0.8  # 过采样比例， 将少数类的样本提高到 over_ratio * max_samples\n",
    "for i, num in enumerate(num_train):\n",
    "    sampling_strategy[i] = int(num) if num == max_samples else int(max_samples * over_ratio)\n",
    "print(f\"over sampler: {num_original} => {sampling_strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、随机过采样\n",
    "+ 设置随机数种子random_state为常数，使得采样过程可重现\n",
    "+ sampling_strategy={0: 26512, 1: 15000, 2: 15000} => 生成指定样本数数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18558, 14846, 14846])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=0)\n",
    "x_ros, y_ros = ros.fit_resample(x_train, y_train)\n",
    "count_class_samples(y_ros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、少数类的插值采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48250,), (27105,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smo = SMOTE(sampling_strategy=sampling_strategy, random_state=0)\n",
    "x_smo, y_smo = smo.fit_resample(x_train, y_train)\n",
    "y_smo.shape, y_train.shape, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、边界类样本采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48250,), (27105,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsmo = BorderlineSMOTE(kind='borderline-1', sampling_strategy=sampling_strategy,\n",
    "                       random_state=42)   # 'borderline-1', 'borderline-2'\n",
    "x_bsmo, y_bsmo = bsmo.fit_resample(x_train, y_train)\n",
    "y_bsmo.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、自适应合成抽样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48337,), (27105,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana = ADASYN(sampling_strategy=sampling_strategy, random_state=0)\n",
    "x_ana, y_ana = ana.fit_resample(x_train, y_train)\n",
    "y_ana.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、欠采样策略——数据不平衡\n",
    "+ 对样本数较多的类别，随机丢弃一部分样本，使其样本数解决样本数少的类别，从而达到数据平衡\n",
    "+ 缺点：没有充分利用多数类的样本信息，如果强行欠采样到与最少样本类别一致，可能会丢失大量的样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler, NearMiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、 随机丢弃一部分样本\n",
    "+ sampling_strategy 为默认值 'auto'时， 将所有类别的样本数欠采样到与样本数最少的类别相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4863,), array([1621, 1621, 1621]), array([18558,  1621,  6926]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=0)\n",
    "x_rus, y_rus = rus.fit_resample(x_train, y_train)\n",
    "y_rus.shape, count_class_samples(y_rus), count_class_samples(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、根据 k-means 中心生成\n",
    "+ 计算量比较大，生成时间较长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1621, 1621, 1621]), array([18558,  1621,  6926]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = ClusterCentroids(sampling_strategy='auto', random_state=0)\n",
    "x_cc, y_cc = cc.fit_resample(x_train, y_train)\n",
    "count_class_samples(y_cc), count_class_samples(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、根据邻近样本规则进行下采样，含三种方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1621, 1621, 1621]), array([18558,  1621,  6926]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nm = NearMiss(sampling_strategy='auto', version=1)\n",
    "x_nm, y_nm = nm.fit_resample(x_train, y_train)\n",
    "count_class_samples(y_nm), count_class_samples(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、获取分类结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、定义分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "class Classifer:\n",
    "    def __init__(self, clf_type='svm', optimizer='sgd', hidden_layer_sizes=(64,), activation='relu'):\n",
    "        \"\"\"\n",
    "        clf_type = 'svm' | 'gpc' | 'mlp'\n",
    "        optimizer = 'lbfgs' | 'sgd' | 'adam'\n",
    "        activation = 'logistic' | 'relu' | 'tanh' | 'identity'\n",
    "        \"\"\"\n",
    "        assert clf_type.lower() in ['svm', 'gpc', 'mlp']\n",
    "        if clf_type.lower() == 'svm':\n",
    "            self.clf = svm.SVC()\n",
    "        elif clf_type.lower() == 'mlp':\n",
    "            self.clf = MLPClassifier(solver=optimizer,  # 'lbfgs', 'sgd', 'adam'\n",
    "                    alpha=1e-5,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes,\n",
    "                    activation=activation,  # logistic, relu, tanh, identity\n",
    "                    random_state=1)\n",
    "            # print(f\"MLP layers => {[layer.shape for layer in self.clf.coefs_]}\")\n",
    "        else:\n",
    "            kernel = 1. * RBF(1.)\n",
    "            self.clf = GaussianProcessClassifier(kernel=kernel, random_state=0)\n",
    "    \n",
    "    def train_test_split(self, x, y, test_size=0.3, stratify=None, verbose=False):\n",
    "        \"\"\"\n",
    "        x, y (array-like): 是要被划分的输入和输出\n",
    "        test_size (float): 测试集的比例, default to 0.3\n",
    "        stractify (array-like): default to None 采用随机抽样, 如果 stractify = y, 采用分层抽样\n",
    "        verbose: 显示划分信息\n",
    "        \"\"\"\n",
    "        data = train_test_split(x_all, y_all, test_size=0.3, stratify=y_all)\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = data\n",
    "        if verbose:\n",
    "            print(f\"{self.x_train.shape=}\\t{self.y_train.shape=}\")\n",
    "            print(f\"{self.x_test.shape=}\\t{self.y_test.shape=}\")\n",
    "            print(f\"train: the number of each class =>\\n{pd.value_counts(self.y_train)}\")\n",
    "            print(f\"test: the number of each class =>\\n{pd.value_counts(self.y_test)}\")\n",
    "\n",
    "    def fit(self, x=None, y=None):\n",
    "        \"\"\"训练分类器, 拟合训练数据\"\"\"\n",
    "        if x is None or y is None:\n",
    "            self.clf.fit(self.x_train, self.y_train)\n",
    "        else:\n",
    "            self.clf.fit(x, y)\n",
    "    \n",
    "    def scores(self, x=None, y=None, num_class=3):\n",
    "        \"\"\"评估模型性能, 得到准确率\"\"\"\n",
    "        x_test = x if x is not None else self.x_test\n",
    "        y_test = y if y is not None else self.y_test\n",
    "        \n",
    "        s_all = round(self.clf.score(x_test, y_test), 3)\n",
    "        scores=dict()\n",
    "        for i in range(num_class):\n",
    "            sample_weight = np.zeros_like(y_test)\n",
    "            sample_weight[y_test == i] = 1\n",
    "            si = round(self.clf.score(x_test, y_test, sample_weight), 3)\n",
    "            scores[i] = si\n",
    "            print(f\"score of class {i:3d} = {si}\")\n",
    "        print(f\"total score = {s_all}\")\n",
    "        return s_all, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、统计数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dict(\n",
    "    original_data=(x_train, y_train),           # 原始数据集\n",
    "    o_RandomOverSampler=(x_ros, y_ros),     # 随机过采样\n",
    "    o_SMOTE=(x_smo, y_smo),                 # SMOTE 插值过采样\n",
    "    o_BorderlineSMOTE=(x_bsmo, y_bsmo),     # 边界SMOTE插值过采样\n",
    "    o_ADASYN=(x_ana, y_ana),                # 自适应合成过采样\n",
    "    u_RandomUnderSampler=(x_rus, y_rus),    # 随机欠采样\n",
    "    u_ClusterCentroids=(x_cc, y_cc),        # 聚类中心欠采样\n",
    "    u_NearMiss=(x_nm, y_nm),                # 最近邻欠采样\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------data_name='original_data'-------------\n",
      "self.x_train.shape=(27105, 37)\tself.y_train.shape=(27105,)\n",
      "self.x_test.shape=(11617, 37)\tself.y_test.shape=(11617,)\n",
      "train: the number of each class =>\n",
      "0    18558\n",
      "2     6926\n",
      "1     1621\n",
      "dtype: int64\n",
      "test: the number of each class=>\n",
      "0    7954\n",
      "2    2968\n",
      "1     695\n",
      "dtype: int64\n",
      "score of class   0 = 0.996\n",
      "score of class   1 = 0.131\n",
      "score of class   2 = 0.686\n",
      "total score = 0.865\n",
      "---------data_name='o_RandomOverSampler'----------\n",
      "self.x_train.shape=(27105, 37)\tself.y_train.shape=(27105,)\n",
      "self.x_test.shape=(11617, 37)\tself.y_test.shape=(11617,)\n",
      "train: the number of each class =>\n",
      "0    18558\n",
      "2     6926\n",
      "1     1621\n",
      "dtype: int64\n",
      "test: the number of each class=>\n",
      "0    7954\n",
      "2    2968\n",
      "1     695\n",
      "dtype: int64\n",
      "score of class   0 = 0.997\n",
      "score of class   1 = 0.112\n",
      "score of class   2 = 0.696\n",
      "total score = 0.868\n",
      "---------------data_name='o_SMOTE'----------------\n",
      "self.x_train.shape=(27105, 37)\tself.y_train.shape=(27105,)\n",
      "self.x_test.shape=(11617, 37)\tself.y_test.shape=(11617,)\n",
      "train: the number of each class =>\n",
      "0    18558\n",
      "2     6926\n",
      "1     1621\n",
      "dtype: int64\n",
      "test: the number of each class=>\n",
      "0    7954\n",
      "2    2968\n",
      "1     695\n",
      "dtype: int64\n",
      "score of class   0 = 0.997\n",
      "score of class   1 = 0.117\n",
      "score of class   2 = 0.688\n",
      "total score = 0.866\n",
      "----------data_name='o_BorderlineSMOTE'-----------\n",
      "self.x_train.shape=(27105, 37)\tself.y_train.shape=(27105,)\n",
      "self.x_test.shape=(11617, 37)\tself.y_test.shape=(11617,)\n",
      "train: the number of each class =>\n",
      "0    18558\n",
      "2     6926\n",
      "1     1621\n",
      "dtype: int64\n",
      "test: the number of each class=>\n",
      "0    7954\n",
      "2    2968\n",
      "1     695\n",
      "dtype: int64\n",
      "score of class   0 = 0.996\n",
      "score of class   1 = 0.112\n",
      "score of class   2 = 0.689\n",
      "total score = 0.865\n",
      "---------------data_name='o_ADASYN'---------------\n",
      "self.x_train.shape=(27105, 37)\tself.y_train.shape=(27105,)\n",
      "self.x_test.shape=(11617, 37)\tself.y_test.shape=(11617,)\n",
      "train: the number of each class =>\n",
      "0    18558\n",
      "2     6926\n",
      "1     1621\n",
      "dtype: int64\n",
      "test: the number of each class=>\n",
      "0    7954\n",
      "2    2968\n",
      "1     695\n",
      "dtype: int64\n",
      "score of class   0 = 0.997\n",
      "score of class   1 = 0.128\n",
      "score of class   2 = 0.701\n",
      "total score = 0.87\n",
      "---------data_name='u_RandomUnderSampler'---------\n",
      "self.x_train.shape=(27105, 37)\tself.y_train.shape=(27105,)\n",
      "self.x_test.shape=(11617, 37)\tself.y_test.shape=(11617,)\n",
      "train: the number of each class =>\n",
      "0    18558\n",
      "2     6926\n",
      "1     1621\n",
      "dtype: int64\n",
      "test: the number of each class=>\n",
      "0    7954\n",
      "2    2968\n",
      "1     695\n",
      "dtype: int64\n",
      "score of class   0 = 0.998\n",
      "score of class   1 = 0.112\n",
      "score of class   2 = 0.694\n",
      "total score = 0.867\n",
      "----------data_name='u_ClusterCentroids'----------\n",
      "self.x_train.shape=(27105, 37)\tself.y_train.shape=(27105,)\n",
      "self.x_test.shape=(11617, 37)\tself.y_test.shape=(11617,)\n",
      "train: the number of each class =>\n",
      "0    18558\n",
      "2     6926\n",
      "1     1621\n",
      "dtype: int64\n",
      "test: the number of each class=>\n",
      "0    7954\n",
      "2    2968\n",
      "1     695\n",
      "dtype: int64\n",
      "score of class   0 = 0.998\n",
      "score of class   1 = 0.122\n",
      "score of class   2 = 0.686\n",
      "total score = 0.866\n",
      "--------------data_name='u_NearMiss'--------------\n",
      "self.x_train.shape=(27105, 37)\tself.y_train.shape=(27105,)\n",
      "self.x_test.shape=(11617, 37)\tself.y_test.shape=(11617,)\n",
      "train: the number of each class =>\n",
      "0    18558\n",
      "2     6926\n",
      "1     1621\n",
      "dtype: int64\n",
      "test: the number of each class=>\n",
      "0    7954\n",
      "2    2968\n",
      "1     695\n",
      "dtype: int64\n",
      "score of class   0 = 0.997\n",
      "score of class   1 = 0.144\n",
      "score of class   2 = 0.697\n",
      "total score = 0.87\n"
     ]
    }
   ],
   "source": [
    "svm_results = dict()  \n",
    "for data_name, dt in train_data.items(): \n",
    "    clf = Classifer(clf_type='svm')  \n",
    "    print(f\"{data_name=}\".center(50, '-'))  \n",
    "    # clf.train_test_split(dt[0], dt[1], verbose=True)   # 划分训练集和测试集\n",
    "    # clf.fit() \n",
    "    # svm_results[data_name] = clf.scores() \n",
    "    clf.fit(dt[0], dt[1]) \n",
    "    svm_results[data_name] = clf.scores(x_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------data_name='original_data'-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\Anaconda3\\envs\\p39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of class   0 = 0.987\n",
      "score of class   1 = 0.292\n",
      "score of class   2 = 0.774\n",
      "total score = 0.891\n",
      "---------data_name='o_RandomOverSampler'----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\Anaconda3\\envs\\p39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of class   0 = 0.92\n",
      "score of class   1 = 0.866\n",
      "score of class   2 = 0.773\n",
      "total score = 0.879\n",
      "---------------data_name='o_SMOTE'----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\Anaconda3\\envs\\p39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of class   0 = 0.899\n",
      "score of class   1 = 0.832\n",
      "score of class   2 = 0.813\n",
      "total score = 0.873\n",
      "----------data_name='o_BorderlineSMOTE'-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\Anaconda3\\envs\\p39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of class   0 = 0.874\n",
      "score of class   1 = 0.875\n",
      "score of class   2 = 0.592\n",
      "total score = 0.802\n",
      "---------------data_name='o_ADASYN'---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\Anaconda3\\envs\\p39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of class   0 = 0.879\n",
      "score of class   1 = 0.845\n",
      "score of class   2 = 0.698\n",
      "total score = 0.831\n",
      "---------data_name='u_RandomUnderSampler'---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\Anaconda3\\envs\\p39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of class   0 = 0.919\n",
      "score of class   1 = 0.714\n",
      "score of class   2 = 0.502\n",
      "total score = 0.8\n",
      "----------data_name='u_ClusterCentroids'----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\Anaconda3\\envs\\p39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of class   0 = 0.868\n",
      "score of class   1 = 0.788\n",
      "score of class   2 = 0.378\n",
      "total score = 0.738\n",
      "--------------data_name='u_NearMiss'--------------\n",
      "score of class   0 = 0.814\n",
      "score of class   1 = 0.679\n",
      "score of class   2 = 0.48\n",
      "total score = 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\Anaconda3\\envs\\p39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp_results = dict()\n",
    "for data_name, dt in data.items():  \n",
    "    clf = Classifer(clf_type='mlp', \n",
    "                optimizer='sgd', \n",
    "                hidden_layer_sizes=(64,), \n",
    "                activation='relu')  \n",
    "    print(f\"{data_name=}\".center(50, '-'))  \n",
    "    # clf.train_test_split(dt[0], dt[1], verbose=True)   # 划分训练集和测试集\n",
    "    # clf.fit()  \n",
    "    # mlp_results[data_name] = clf.scores()  \n",
    "    clf.fit(dt[0], dt[1]) \n",
    "    mlp_results[data_name] = clf.scores(x_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、GPC\n",
    "+ 内存占用太大，一跑就死机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------data_name='original_data'-------------\n"
     ]
    }
   ],
   "source": [
    "# gpc_results = dict()\n",
    "# for data_name, dt in data.items():\n",
    "#     clf = Classifer(clf_type='gpc')  \n",
    "#     print(f\"{data_name=}\".center(50, '-'))\n",
    "#     # clf.train_test_split(dt[0], dt[1], verbose=True)   # 划分训练集和测试集\n",
    "#     # clf.fit()\n",
    "#     # gpc_results[data_name] = clf.scores()\n",
    "#     clf.fit(dt[0], dt[1]) \n",
    "#     gpc_results[data_name] = clf.scores(x_test, y_test) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('p39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2977251e1b2ca22a3cfc09d5f408daba24a1809e852c29e73dfa9b1ba1933376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
